# ğŸ“ PPO Agent Playing Atari Pong

This project trains a reinforcement learning agent to play **Atari Pong**
using **Proximal Policy Optimization (PPO)** and the
**Arcade Learning Environment (ALE)**.

The agent learns directly from pixels and is able to sustain long rallies
and demonstrate non-random behavior after extended training.

---

## ğŸ® Demo

After **1 million timesteps**, the agent:
- Tracks the ball consistently
- Maintains long rallies
- Demonstrates learned paddle control

![Pong PPO Agent](docs/pong_demo.gif)

---

## ğŸ§  Method

- Algorithm: **PPO (Proximal Policy Optimization)**
- Observation space: stacked grayscale frames (84Ã—84)
- Action space: discrete Atari actions
- Training environment: `ALE/Pong-v5`

---

## ğŸ› ï¸ Tech Stack

- Python 3.11
- Gymnasium
- Arcade Learning Environment (ALE)
- Stable-Baselines3
- PyTorch
- NumPy
- OpenCV

---

## ğŸš€ Training

```bash
python train_pong.py

ğŸ“‚ Project Structure
pong-rl-agent/
â”œâ”€â”€ train_pong.py
â”œâ”€â”€ eval_pong.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ ppo_pong_1M.zip
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ pong_demo.gif
â””â”€â”€ README.md



